{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHIR Test Plan Generator\n",
    "\n",
    "This notebook generates a consolidated test plan markdown file from FHIR Implementation Guide requirements. The output serves as a complete specification that can be used by an LLM to generate executable test scripts.\n",
    "\n",
    "#### What it does\n",
    "\n",
    "- Processes each requirement from a markdown input file\n",
    "- Based on the IG capability statement, generates comprehensive test specifications including:\n",
    "  - Testability assessment (Automatically testable/assertion/not testable) and level of complexity\n",
    "  - Implementation strategy with specific FHIR operations\n",
    "  - Required pre-reqs, inputs including required FHIR resources, and expected outputs\n",
    "  - Validation criteria\n",
    "- Creates a single, well-structured markdown file with a table of contents\n",
    "\n",
    "#### How to use\n",
    "\n",
    "1. **Setup**: Individual cert setup may need to be modified in `setup_clients()` function of the llm_utils.py file. API keys should be in .env file. Make sure you have API keys for at least one of:\n",
    "   - Anthropic Claude (`ANTHROPIC_API_KEY`)\n",
    "   - Google Gemini (`GEMINI_API_KEY`) \n",
    "   - OpenAI GPT-4 (`OPENAI_API_KEY`)\n",
    "\n",
    "2. **Input**: A markdown file with requirements in the following format:\n",
    "   ```markdown\n",
    "   # REQ-ID\n",
    "   **Summary**: Requirement summary\n",
    "   **Description**: Detailed description\n",
    "   **Verification**: Test approach\n",
    "   **Actor**: System component responsible\n",
    "   **Conformance**: SHALL/SHOULD/MAY\n",
    "   **Conditional**: True/False\n",
    "   **Source**: Original requirement sources\n",
    "   ---\n",
    "   ```\n",
    "   And an IG capability statement file in markdown format.\n",
    "\n",
    "3. **Run**: Execute the `run_test_plan_generator()` function and follow the prompts:\n",
    "   - Specify the input directory or use the default\n",
    "   - Select which requirements list file to use or provide the path to a requirements file\n",
    "   - Enter the Implementation Guide name\n",
    "   - Specify the output directory, or use the default\n",
    "   - Select which LLM to use\n",
    "\n",
    "4. **Output**: A single markdown file will be generated with the format:\n",
    "   `[llm]_test_plan_[timestamp].md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 13:24:58,450 - __main__ - INFO - Current working directory: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev\n",
      "2025-06-25 13:24:58,451 - __main__ - INFO - Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "2025-06-25 13:24:58,451 - __main__ - INFO - Default input directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs_output\n",
      "2025-06-25 13:24:58,451 - __main__ - INFO - Default output directory: /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_plan_output\n",
      "2025-06-25 13:24:58,452 - __main__ - INFO - Default capability statement directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level to project root\n",
    "CURRENT_DIR = Path.cwd()  # Current working directory (test_kit_dev)\n",
    "DEFAULT_INPUT_DIR = Path(PROJECT_ROOT, 'reqs_extraction', 'revised_reqs_output')  # Default input directory\n",
    "DEFAULT_OUTPUT_DIR = Path(CURRENT_DIR, 'test_plan_output')  # Default output directory\n",
    "DEFAULT_CAPABILITY_DIR = Path(PROJECT_ROOT, 'full-ig', 'markdown7_cleaned')  # Default capability statement directory\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "DEFAULT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Log directory information\n",
    "logger.info(f\"Current working directory: {CURRENT_DIR}\")\n",
    "logger.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logger.info(f\"Default input directory: {DEFAULT_INPUT_DIR}\")\n",
    "logger.info(f\"Default output directory: {DEFAULT_OUTPUT_DIR}\")\n",
    "logger.info(f\"Default capability statement directory: {DEFAULT_CAPABILITY_DIR}\")\n",
    "\n",
    "# Function to find capability statement files\n",
    "def find_capability_statement_files(directory=DEFAULT_CAPABILITY_DIR):\n",
    "    \"\"\"Find files containing 'CapabilityStatement' in the filename\"\"\"\n",
    "    if not directory.exists():\n",
    "        logger.warning(f\"Capability statement directory {directory} does not exist\")\n",
    "        return []\n",
    "    \n",
    "    capability_files = list(directory.glob(\"*CapabilityStatement*.md\"))\n",
    "    capability_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "    return capability_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "module_path = os.path.join(PROJECT_ROOT, 'llm_utils.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm_utils\", module_path)\n",
    "llm_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(llm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 13:24:59,418 - root - INFO - Prompt environment set up at: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "2025-06-25 13:24:59,418 - root - INFO - Using prompts directory: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "2025-06-25 13:24:59,418 - root - INFO - Test plan prompt: /Users/ceadams/Documents/onclaive/onclaive/prompts/test_plan.md\n",
      "2025-06-25 13:24:59,419 - root - INFO - Requirement grouping prompt: /Users/ceadams/Documents/onclaive/onclaive/prompts/requirement_grouping.md\n"
     ]
    }
   ],
   "source": [
    "# Import prompt utilities\n",
    "prompt_utils_path = os.path.join(PROJECT_ROOT, 'prompt_utils.py')\n",
    "spec = importlib.util.spec_from_file_location(\"prompt_utils\", prompt_utils_path)\n",
    "prompt_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(prompt_utils)\n",
    "\n",
    "# Setup the prompt environment\n",
    "prompt_env = prompt_utils.setup_prompt_environment(PROJECT_ROOT)\n",
    "PROMPT_DIR = prompt_env[\"prompt_dir\"]\n",
    "TEST_PLAN_PATH = prompt_env[\"test_plan_gen_path\"]\n",
    "REQUIREMENT_GROUPING_PATH = prompt_env[\"requirement_grouping_path\"]\n",
    "\n",
    "logging.info(f\"Using prompts directory: {PROMPT_DIR}\")\n",
    "logging.info(f\"Test plan prompt: {TEST_PLAN_PATH}\")\n",
    "logging.info(f\"Requirement grouping prompt: {REQUIREMENT_GROUPING_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Configuration & Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts for test generation\n",
    "SYSTEM_PROMPT = \"\"\"You are a specialized FHIR testing engineer with expertise in healthcare interoperability.\n",
    "Your task is to analyze FHIR Implementation Guide requirements and generate practical, implementable test specifications.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_test_plan_prompt(requirement: str, capability_info: str) -> str:\n",
    "    \"\"\"Load the test plan prompt from file and format it with requirement and capability info\"\"\"\n",
    "    return prompt_utils.load_prompt(\n",
    "        TEST_PLAN_PATH,\n",
    "        requirement=requirement,\n",
    "        capability_info=capability_info\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_requirement_grouping_prompt(requirement: str) -> str:\n",
    "    \"\"\"Load the requirement grouping prompt from file and format it with requirement info\"\"\"\n",
    "    return prompt_utils.load_prompt(\n",
    "        REQUIREMENT_GROUPING_PATH,\n",
    "        requirement=requirement\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capability Statement Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_capability_statement(file_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse a FHIR Capability Statement markdown file into a structured dictionary\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Capability Statement markdown file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing structured Capability Statement information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract resource capabilities\n",
    "    resource_sections = {}\n",
    "    \n",
    "    # Find resource sections - they typically start with \"#### ResourceName\"\n",
    "    resource_matches = re.finditer(r'#### ([A-Za-z]+)\\n', content)\n",
    "    \n",
    "    for match in resource_matches:\n",
    "        resource_name = match.group(1)\n",
    "        start_pos = match.start()\n",
    "        \n",
    "        # Find the next resource section or end of document\n",
    "        next_match = re.search(r'#### ([A-Za-z]+)\\n', content[start_pos + len(match.group(0)):])\n",
    "        if next_match:\n",
    "            end_pos = start_pos + len(match.group(0)) + next_match.start()\n",
    "            resource_section = content[start_pos:end_pos]\n",
    "        else:\n",
    "            resource_section = content[start_pos:]\n",
    "        \n",
    "        # Extract specific capabilities\n",
    "        search_params = []\n",
    "        search_param_section = re.search(r'Search Parameter Summary:.*?\\| Conformance \\| Parameter \\| Type \\| Example \\|\\n\\| --- \\| --- \\| --- \\| --- \\|(.*?)(?:\\n\\n---|\\Z)', \n",
    "                                       resource_section, re.DOTALL)\n",
    "        \n",
    "        if search_param_section:\n",
    "            param_lines = search_param_section.group(1).strip().split('\\n')\n",
    "            for line in param_lines:\n",
    "                if '|' in line:\n",
    "                    parts = [p.strip() for p in line.split('|')]\n",
    "                    if len(parts) >= 5 and parts[1] and parts[2]:\n",
    "                        conformance = parts[1].replace('**', '')\n",
    "                        param_name = parts[2]\n",
    "                        param_type = parts[3]\n",
    "                        search_params.append({\n",
    "                            'name': param_name,\n",
    "                            'type': param_type,\n",
    "                            'conformance': conformance\n",
    "                        })\n",
    "        \n",
    "        # Extract supported operations\n",
    "        operations = []\n",
    "        operations_section = re.search(r'Supported Operations:(.*?)(?:\\n\\n|\\Z)', resource_section, re.DOTALL)\n",
    "        if operations_section:\n",
    "            op_lines = operations_section.group(1).strip().split('\\n')\n",
    "            for line in op_lines:\n",
    "                if line.strip():\n",
    "                    operations.append(line.strip())\n",
    "        \n",
    "        # Extract includes and revincludes\n",
    "        includes = []\n",
    "        includes_section = re.search(r'A Server \\*\\*SHALL\\*\\* be capable of supporting the following \\_includes:(.*?)(?:\\n\\n|\\Z)', \n",
    "                                   resource_section, re.DOTALL)\n",
    "        if includes_section:\n",
    "            include_lines = includes_section.group(1).strip().split('\\n')\n",
    "            for line in include_lines:\n",
    "                if line.strip():\n",
    "                    include_match = re.search(r'([A-Za-z]+):([A-Za-z\\-]+)', line)\n",
    "                    if include_match:\n",
    "                        includes.append(f\"{include_match.group(1)}:{include_match.group(2)}\")\n",
    "        \n",
    "        revincludes = []\n",
    "        revincludes_section = re.search(r'A Server \\*\\*SHALL\\*\\* be capable of supporting the following \\_revincludes:(.*?)(?:\\n\\n|\\Z)', \n",
    "                                      resource_section, re.DOTALL)\n",
    "        if revincludes_section:\n",
    "            revinclude_lines = revincludes_section.group(1).strip().split('\\n')\n",
    "            for line in revinclude_lines:\n",
    "                if line.strip():\n",
    "                    revinclude_match = re.search(r'([A-Za-z]+):([A-Za-z\\-]+)', line)\n",
    "                    if revinclude_match:\n",
    "                        revincludes.append(f\"{revinclude_match.group(1)}:{revinclude_match.group(2)}\")\n",
    "        \n",
    "        resource_sections[resource_name] = {\n",
    "            'search_parameters': search_params,\n",
    "            'operations': operations,\n",
    "            'includes': includes,\n",
    "            'revincludes': revincludes\n",
    "        }\n",
    "    \n",
    "    # Extract general capabilities\n",
    "    general_capabilities = {}\n",
    "    general_section = re.search(r'### FHIR RESTful Capabilities(.*?)(?:###|$)', content, re.DOTALL)\n",
    "    if general_section:\n",
    "        shall_match = re.search(r'The Plan-Net Server \\*\\*SHALL\\*\\*:(.*?)(?:The Plan-Net Server \\*\\*SHOULD\\*\\*:|\\n\\n\\*\\*Security:\\*\\*|\\Z)', \n",
    "                              general_section.group(1), re.DOTALL)\n",
    "        should_match = re.search(r'The Plan-Net Server \\*\\*SHOULD\\*\\*:(.*?)(?:\\n\\n\\*\\*Security:\\*\\*|\\Z)', \n",
    "                               general_section.group(1), re.DOTALL)\n",
    "        \n",
    "        if shall_match:\n",
    "            shall_items = re.findall(r'\\d+\\.\\s*(.*?)(?:\\n\\d+\\.|\\Z)', shall_match.group(1), re.DOTALL)\n",
    "            general_capabilities['SHALL'] = [item.strip() for item in shall_items]\n",
    "        \n",
    "        if should_match:\n",
    "            should_items = re.findall(r'\\d+\\.\\s*(.*?)(?:\\n\\d+\\.|\\Z)', should_match.group(1), re.DOTALL)\n",
    "            general_capabilities['SHOULD'] = [item.strip() for item in should_items]\n",
    "    \n",
    "    return {\n",
    "        'resources': resource_sections,\n",
    "        'general_capabilities': general_capabilities\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_capability_info(requirement: Dict[str, str], capability_statement: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Extract relevant capability statement information for a specific requirement\n",
    "    \n",
    "    Args:\n",
    "        requirement: Requirement dictionary\n",
    "        capability_statement: Parsed capability statement\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string with relevant capability information\n",
    "    \"\"\"\n",
    "    # Determine which resource types are relevant to this requirement\n",
    "    requirement_text = f\"{requirement.get('description', '')} {requirement.get('summary', '')}\"\n",
    "    resource_types = []\n",
    "    \n",
    "    # plan net resource types\n",
    "    fhir_resources = [\n",
    "        \"Patient\", \"Practitioner\", \"Organization\", \"Location\", \"Endpoint\", \n",
    "        \"HealthcareService\", \"PractitionerRole\", \"OrganizationAffiliation\",\n",
    "        \"InsurancePlan\", \"Network\"\n",
    "    ]\n",
    "    \n",
    "    # Check if requirement mentions specific resources\n",
    "    for resource in fhir_resources:\n",
    "        if resource in requirement_text:\n",
    "            resource_types.append(resource)\n",
    "    \n",
    "    # If no specific resources found, check for general requirements\n",
    "    if not resource_types:\n",
    "        # If it's a server requirement\n",
    "        if \"Server\" in requirement.get('actor', ''):\n",
    "            resource_types = [\"General Server Capabilities\"]\n",
    "        # If it's a client requirement\n",
    "        elif \"Client\" in requirement.get('actor', '') or \"Application\" in requirement.get('actor', ''):\n",
    "            resource_types = [\"General Client Capabilities\"]\n",
    "    \n",
    "    # Build relevant capability information\n",
    "    relevant_info = \"### Applicable Capability Statement Information\\n\\n\"\n",
    "    \n",
    "    # Add general capabilities\n",
    "    relevant_info += \"#### General Capabilities\\n\"\n",
    "    if \"general_capabilities\" in capability_statement:\n",
    "        for level in [\"SHALL\", \"SHOULD\"]:\n",
    "            if level in capability_statement[\"general_capabilities\"]:\n",
    "                relevant_info += f\"\\n**{level}**:\\n\"\n",
    "                for item in capability_statement[\"general_capabilities\"][level]:\n",
    "                    relevant_info += f\"- {item}\\n\"\n",
    "    \n",
    "    # Add resource-specific capabilities\n",
    "    for resource_type in resource_types:\n",
    "        if resource_type in capability_statement.get(\"resources\", {}):\n",
    "            resource_info = capability_statement[\"resources\"][resource_type]\n",
    "            \n",
    "            relevant_info += f\"\\n#### {resource_type} Resource Capabilities\\n\"\n",
    "            \n",
    "            # Add search parameters\n",
    "            if resource_info.get(\"search_parameters\"):\n",
    "                relevant_info += \"\\n**Supported Search Parameters**:\\n\"\n",
    "                for param in resource_info[\"search_parameters\"]:\n",
    "                    relevant_info += f\"- {param['name']} ({param['type']}): {param['conformance']}\\n\"\n",
    "            \n",
    "            # Add operations\n",
    "            if resource_info.get(\"operations\"):\n",
    "                relevant_info += \"\\n**Supported Operations**:\\n\"\n",
    "                for op in resource_info[\"operations\"]:\n",
    "                    relevant_info += f\"- {op}\\n\"\n",
    "            \n",
    "            # Add includes\n",
    "            if resource_info.get(\"includes\"):\n",
    "                relevant_info += \"\\n**Supported _includes**:\\n\"\n",
    "                for include in resource_info[\"includes\"]:\n",
    "                    relevant_info += f\"- {include}\\n\"\n",
    "            \n",
    "            # Add revincludes\n",
    "            if resource_info.get(\"revincludes\"):\n",
    "                relevant_info += \"\\n**Supported _revincludes**:\\n\"\n",
    "                for revinclude in resource_info[\"revincludes\"]:\n",
    "                    relevant_info += f\"- {revinclude}\\n\"\n",
    "    \n",
    "    return relevant_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_requirements_file(file_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse an INCOSE requirements markdown file into a structured list of requirements\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the requirements markdown file\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing structured requirement information\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Split by requirement sections (separated by ---)\n",
    "    req_sections = content.split('---')\n",
    "    \n",
    "    requirements = []\n",
    "    for section in req_sections:\n",
    "        if not section.strip():\n",
    "            continue\n",
    "            \n",
    "        # Parse requirement data\n",
    "        req_data = {}\n",
    "        \n",
    "        # Extract ID from format \"# REQ-XX\"\n",
    "        id_match = re.search(r'#\\s+([A-Z0-9\\-]+)', section)\n",
    "        if id_match:\n",
    "            req_data['id'] = id_match.group(1)\n",
    "        \n",
    "        # Extract other fields\n",
    "        for field in ['Summary', 'Description', 'Verification', 'Actor', 'Conformance', 'Conditional', 'Source']:\n",
    "            pattern = rf'\\*\\*{field}\\*\\*:\\s*(.*?)(?:\\n\\*\\*|\\n---|\\\\Z)'\n",
    "            field_match = re.search(pattern, section, re.DOTALL)\n",
    "            if field_match:\n",
    "                req_data[field.lower()] = field_match.group(1).strip()\n",
    "        \n",
    "        if req_data:\n",
    "            requirements.append(req_data)\n",
    "    \n",
    "    return requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reqs = parse_requirements_file(\"../pipeline/checkpoints/requirements_extraction/claude_reqs_list_v1_20250429_081756.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse_requirements_file.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_requirement_for_prompt(requirement: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    Format a requirement dictionary into markdown for inclusion in prompts\n",
    "    \n",
    "    Args:\n",
    "        requirement: Requirement dictionary\n",
    "        \n",
    "    Returns:\n",
    "        Formatted markdown string\n",
    "    \"\"\"\n",
    "    formatted = f\"# {requirement.get('id', 'UNKNOWN-ID')}\\n\"\n",
    "    formatted += f\"**Summary**: {requirement.get('summary', '')}\\n\"\n",
    "    formatted += f\"**Description**: {requirement.get('description', '')}\\n\"\n",
    "    formatted += f\"**Verification**: {requirement.get('verification', '')}\\n\"\n",
    "    formatted += f\"**Actor**: {requirement.get('actor', '')}\\n\"\n",
    "    formatted += f\"**Conformance**: {requirement.get('conformance', '')}\\n\"\n",
    "    formatted += f\"**Conditional**: {requirement.get('conditional', '')}\\n\"\n",
    "    formatted += f\"**Source**: {requirement.get('source', '')}\\n\"\n",
    "    \n",
    "    return formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'summary'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"**Summary**:\"[2:-3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_requirement_group(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    requirement: Dict[str, str],\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Identify the appropriate group for a requirement using LLM only\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirement: Requirement dictionary\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Identified group name from LLM\n",
    "    \"\"\"\n",
    "    # use LLM to identify group based on prompt\n",
    "    logger.info(f\"Identifying group for requirement {requirement.get('id', 'unknown')} using {api_type}...\")\n",
    "    \n",
    "    # Format requirement as markdown\n",
    "    formatted_req = format_requirement_for_prompt(requirement)\n",
    "    \n",
    "    # Retrieve prompt with the requirement using external file\n",
    "    prompt = get_requirement_grouping_prompt(formatted_req)\n",
    "    \n",
    "    # Make the API request with simplified system prompt\n",
    "    group_system_prompt = \"You are a FHIR expert who categorizes requirements by their functional or resource type.\"\n",
    "    group_name = llm_utils.make_llm_request(client, api_type, prompt, group_system_prompt, rate_limit_func).strip()\n",
    "    \n",
    "    # Clean up response (in case model returns extra text)\n",
    "    if '\\n' in group_name:\n",
    "        group_name = group_name.split('\\n')[0].strip()\n",
    "    \n",
    "    return group_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Plan Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_specification_with_capability(\n",
    "    client, \n",
    "    api_type: str,\n",
    "    requirement: Dict[str, str],\n",
    "    capability_statement: Dict[str, Any],\n",
    "    rate_limit_func\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive test specification for a single requirement, considering capability statement\n",
    "    \n",
    "    Args:\n",
    "        client: The API client\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirement: Requirement dictionary\n",
    "        capability_statement: Parsed capability statement\n",
    "        rate_limit_func: Function to check rate limits\n",
    "        \n",
    "    Returns:\n",
    "        Test specification for the requirement\n",
    "    \"\"\"\n",
    "    logger.info(f\"Generating test specification for {requirement.get('id', 'unknown')} using {api_type}...\")\n",
    "    \n",
    "    # Format requirement as markdown\n",
    "    formatted_req = format_requirement_for_prompt(requirement)\n",
    "    \n",
    "    # Extract relevant capability information\n",
    "    capability_info = extract_relevant_capability_info(requirement, capability_statement)\n",
    "    \n",
    "    # Create prompt with the requirement and capability information using external file\n",
    "    prompt = get_test_plan_prompt(formatted_req, capability_info)\n",
    "    \n",
    "    # Make the API request\n",
    "    return llm_utils.make_llm_request(client, api_type, prompt, SYSTEM_PROMPT, rate_limit_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_consolidated_test_plan(\n",
    "    api_type: str,\n",
    "    requirements_file: str,\n",
    "    capability_statement_file: str = None,\n",
    "    ig_name: str = \"FHIR Implementation Guide\",\n",
    "    output_dir: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process requirements and generate a consolidated test plan\n",
    "    \n",
    "    Args:\n",
    "        api_type: API type (claude, gemini, gpt)\n",
    "        requirements_file: Path to requirements markdown file\n",
    "        capability_statement_file: Path to capability statement markdown file (optional)\n",
    "        ig_name: Name of the Implementation Guide\n",
    "        output_dir: Directory for output files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing path to output file\n",
    "    \"\"\"\n",
    "    # Use default output directory if none provided\n",
    "    if output_dir is None:\n",
    "        output_dir = DEFAULT_OUTPUT_DIR\n",
    "    else:\n",
    "        # Ensure output_dir is a Path object\n",
    "        if not isinstance(output_dir, Path):\n",
    "            output_dir = Path(output_dir)\n",
    "    \n",
    "    logger.info(f\"Starting test plan generation with {api_type} for {ig_name}\")\n",
    "    \n",
    "    # Initialize API clients and rate limiters\n",
    "    clients = llm_utils.setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = llm_utils.API_CONFIGS[api_type]\n",
    "    rate_limiter = llm_utils.create_rate_limiter()\n",
    "    \n",
    "    def check_limits():\n",
    "        llm_utils.check_rate_limits(rate_limiter, api_type)\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    try:\n",
    "        # Parse requirements from file\n",
    "        requirements = parse_requirements_file(requirements_file)\n",
    "        logger.info(f\"Parsed {len(requirements)} requirements from {requirements_file}\")\n",
    "        \n",
    "        # Parse capability statement if provided\n",
    "        capability_statement = None\n",
    "        if capability_statement_file and os.path.exists(capability_statement_file):\n",
    "            capability_statement = parse_capability_statement(capability_statement_file)\n",
    "            logger.info(f\"Parsed capability statement from {capability_statement_file}\")\n",
    "        \n",
    "        # Identify groups for each requirement\n",
    "        req_groups = {}\n",
    "        for req in requirements:\n",
    "            req_id = req.get('id', 'UNKNOWN-ID')\n",
    "            req_groups[req_id] = identify_requirement_group(client, api_type, req, check_limits)\n",
    "            # Add small delay to avoid rate limiting\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Group requirements by identified category\n",
    "        grouped_requirements = defaultdict(list)\n",
    "        for req in requirements:\n",
    "            req_id = req.get('id', 'UNKNOWN-ID')\n",
    "            group = req_groups.get(req_id, 'Uncategorized')\n",
    "            grouped_requirements[group].append(req)\n",
    "            \n",
    "        # Log the grouping results\n",
    "        logger.info(f\"Requirements grouped into {len(grouped_requirements)} categories\")\n",
    "        for group, reqs in grouped_requirements.items():\n",
    "            logger.info(f\"Group '{group}': {len(reqs)} requirements\")\n",
    "        \n",
    "        # Update output file path to use Path object\n",
    "        test_plan_path = output_dir / f\"{api_type}_test_plan_{timestamp}.md\"\n",
    "        \n",
    "        # Initialize test plan content\n",
    "        test_plan = f\"# Consolidated Test Plan for {ig_name}\\n\\n\"\n",
    "        test_plan += f\"## Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
    "        test_plan += f\"## API Type: {api_type}\\n\\n\"\n",
    "        \n",
    "        test_plan += \"## Table of Contents\\n\\n\"\n",
    "        \n",
    "        # Add group headers to TOC\n",
    "        for group in sorted(grouped_requirements.keys()):\n",
    "            test_plan += f\"- [{group}](#{group.lower().replace(' ', '-')})\\n\"\n",
    "            for req in grouped_requirements[group]:\n",
    "                req_id = req.get('id', 'UNKNOWN-ID')\n",
    "                req_summary = req.get('summary', 'No summary')\n",
    "                test_plan += f\"  - [{req_id}: {req_summary}](#{req_id.lower()})\\n\"\n",
    "        \n",
    "        # Process each group and its requirements\n",
    "        test_plan += \"\\n## Test Specifications\\n\\n\"\n",
    "        \n",
    "        for group in sorted(grouped_requirements.keys()):\n",
    "            # Add group header with anchor for TOC linking\n",
    "            test_plan += f\"<a id='{group.lower().replace(' ', '-')}'></a>\\n\\n\"\n",
    "            test_plan += f\"## {group}\\n\\n\"\n",
    "            \n",
    "            # Process each requirement in the group\n",
    "            for i, req in enumerate(grouped_requirements[group]):\n",
    "                req_id = req.get('id', 'UNKNOWN-ID')\n",
    "                logger.info(f\"Processing requirement for group '{group}': {req_id}\")\n",
    "                \n",
    "                # Generate test specification with capability statement if available\n",
    "                if capability_statement:\n",
    "                    test_spec = generate_test_specification_with_capability(\n",
    "                        client, api_type, req, capability_statement, check_limits\n",
    "                    )\n",
    "                else:\n",
    "                    # Define a fallback function if needed\n",
    "                    def generate_test_specification(client, api_type, req, check_limits):\n",
    "                        logger.info(f\"Generating basic test specification for {req.get('id', 'unknown')} using {api_type}...\")\n",
    "                        formatted_req = format_requirement_for_prompt(req)\n",
    "                        prompt = f\"\"\"\n",
    "                        Create a test specification for this FHIR Implementation Guide requirement:\n",
    "                        \n",
    "                        {formatted_req}\n",
    "                        \n",
    "                        Include these sections:\n",
    "                        1. Testability Assessment\n",
    "                        2. Complexity\n",
    "                        3. Prerequisites\n",
    "                        4. Required inputs and outputs\n",
    "                        5. Required FHIR Operations\n",
    "                        6. Validation Criteria\n",
    "                        \n",
    "                        Format as markdown with clear headers.\n",
    "                        \"\"\"\n",
    "                        return llm_utils.make_llm_request(client, api_type, prompt, SYSTEM_PROMPT, check_limits)\n",
    "                    \n",
    "                    test_spec = generate_test_specification(client, api_type, req, check_limits)\n",
    "                \n",
    "                # Add to test plan content with proper anchor for TOC linking\n",
    "                test_plan += f\"<a id='{req_id.lower()}'></a>\\n\\n\"\n",
    "                test_plan += f\"### {req_id}: {req.get('summary', 'No summary')}\\n\\n\"\n",
    "                test_plan += f\"**Description**: {req.get('description', '')}\\n\\n\"\n",
    "                test_plan += f\"**Actor**: {req.get('actor', '')}\\n\\n\"\n",
    "                test_plan += f\"**Conformance**: {req.get('conformance', '')}\\n\\n\"\n",
    "                test_plan += f\"{test_spec}\\n\\n\"\n",
    "                test_plan += \"---\\n\\n\"\n",
    "                \n",
    "                # Add delay between requests\n",
    "                if i < len(grouped_requirements[group]) - 1:  # No need to delay after the last request\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "            \n",
    "            # Add spacing between groups\n",
    "            test_plan += \"\\n\\n\"\n",
    "        \n",
    "        # Save consolidated test plan\n",
    "        with open(test_plan_path, 'w') as f:\n",
    "            f.write(test_plan)\n",
    "        logger.info(f\"Consolidated test plan saved to {test_plan_path}\")\n",
    "        \n",
    "        return {\n",
    "            \"requirements_count\": len(requirements),\n",
    "            \"group_count\": len(grouped_requirements),\n",
    "            \"test_plan_path\": str(test_plan_path)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing requirements: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_plan_generator():\n",
    "    start_time = time.time()\n",
    "    # Load environment variables\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get input from user or set default values\n",
    "    print(\"\\nFHIR IG Test Plan Generator\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get input directory or use default\n",
    "    input_dir = input(f\"Enter input directory path or accept default (default '{DEFAULT_INPUT_DIR}'): \") or str(DEFAULT_INPUT_DIR)\n",
    "    input_dir_path = Path(input_dir)\n",
    "    \n",
    "    if not input_dir_path.exists():\n",
    "        print(f\"Warning: Input directory {input_dir} does not exist.\")\n",
    "        requirements_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    else:\n",
    "        # List all markdown files in the input directory\n",
    "        md_files = list(input_dir_path.glob(\"*.md\"))\n",
    "        \n",
    "        if md_files:\n",
    "            # Sort files by modification time (newest first)\n",
    "            md_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)\n",
    "            \n",
    "            # Show only the 10 most recent files\n",
    "            recent_files = md_files[:10]\n",
    "            \n",
    "            print(\"\\nMost recent files:\")\n",
    "            for idx, file in enumerate(recent_files, 1):\n",
    "                # Format the modification time as part of the display\n",
    "                mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "            \n",
    "            # Let user select from the list, see more files, or enter a custom path\n",
    "            print(\"\\nOptions:\")\n",
    "            print(\"- Select a number (1-10) to choose a file\")\n",
    "            print(\"- Enter 'all' to see all files\")\n",
    "            print(\"- Enter a full path to use a specific file\")\n",
    "            \n",
    "            selection = input(\"\\nReview the printed options for choosing a requirements file and enter applicable selection: \")\n",
    "            \n",
    "            if selection.lower() == 'all':\n",
    "                # Show all files with pagination\n",
    "                all_files = md_files\n",
    "                page_size = 20\n",
    "                total_pages = (len(all_files) + page_size - 1) // page_size\n",
    "                \n",
    "                current_page = 1\n",
    "                while current_page <= total_pages:\n",
    "                    start_idx = (current_page - 1) * page_size\n",
    "                    end_idx = min(start_idx + page_size, len(all_files))\n",
    "                    \n",
    "                    print(f\"\\nAll files (page {current_page}/{total_pages}):\")\n",
    "                    for idx, file in enumerate(all_files[start_idx:end_idx], start_idx + 1):\n",
    "                        mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "                        print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "                    \n",
    "                    if current_page < total_pages:\n",
    "                        next_action = input(\"\\nPress Enter for next page, 'q' to select, or enter a number to choose a file: \")\n",
    "                        if next_action.lower() == 'q':\n",
    "                            break\n",
    "                        elif next_action.isdigit() and 1 <= int(next_action) <= len(all_files):\n",
    "                            requirements_file = str(all_files[int(next_action) - 1])\n",
    "                            break\n",
    "                        else:\n",
    "                            current_page += 1\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                if 'requirements_file' not in locals():\n",
    "                    # If we went through all pages without selection\n",
    "                    file_number = input(\"\\nEnter the file number to process: \")\n",
    "                    if file_number.isdigit() and 1 <= int(file_number) <= len(all_files):\n",
    "                        requirements_file = str(all_files[int(file_number) - 1])\n",
    "                    else:\n",
    "                        requirements_file = file_number  # Treat as a custom path\n",
    "            \n",
    "            elif selection.isdigit() and 1 <= int(selection) <= len(recent_files):\n",
    "                requirements_file = str(recent_files[int(selection) - 1])\n",
    "            else:\n",
    "                requirements_file = selection  # Treat as a custom path\n",
    "        else:\n",
    "            print(f\"No markdown files found in {input_dir}\")\n",
    "            requirements_file = input(\"Enter full path to requirements markdown file: \")\n",
    "    \n",
    "    # Check if requirements file exists\n",
    "    if not os.path.exists(requirements_file):\n",
    "        logger.error(f\"Requirements file not found: {requirements_file}\")\n",
    "        print(f\"Error: Requirements file not found at {requirements_file}\")\n",
    "        return\n",
    "    \n",
    "    # Find capability statement files\n",
    "    capability_files = find_capability_statement_files()\n",
    "    \n",
    "    # Get capability statement file path or select from found files\n",
    "    if capability_files:\n",
    "        print(\"\\nFound capability statement files:\")\n",
    "        for idx, file in enumerate(capability_files, 1):\n",
    "            mod_time = datetime.fromtimestamp(file.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
    "            print(f\"{idx}. {file.name} ({mod_time})\")\n",
    "        \n",
    "        print(\"\\nOptions:\")\n",
    "        print(\"- Select a number to choose a capability statement file\")\n",
    "        print(\"- Press Enter to use the most recent file\")\n",
    "        print(\"- Enter 'none' to skip using a capability statement\")\n",
    "        print(\"- Enter a full path to use a specific file\")\n",
    "        \n",
    "        cap_selection = input(\"\\nReview the printed options for choosing a capability statement and enter applicable selection: \")\n",
    "        \n",
    "        if not cap_selection:\n",
    "            # Use the most recent file\n",
    "            capability_statement_file = str(capability_files[0])\n",
    "        elif cap_selection.lower() == 'none':\n",
    "            capability_statement_file = None\n",
    "        elif cap_selection.isdigit() and 1 <= int(cap_selection) <= len(capability_files):\n",
    "            capability_statement_file = str(capability_files[int(cap_selection) - 1])\n",
    "        else:\n",
    "            capability_statement_file = cap_selection  # Treat as a custom path\n",
    "    else:\n",
    "        # No capability statement files found automatically\n",
    "        capability_statement_file = input(\"\\nEnter path to Capability Statement markdown file (optional, press Enter to skip): \")\n",
    "        if not capability_statement_file:\n",
    "            capability_statement_file = None\n",
    "    \n",
    "    # Verify capability statement file exists if provided\n",
    "    if capability_statement_file and not os.path.exists(capability_statement_file):\n",
    "        logger.warning(f\"Capability Statement file not found: {capability_statement_file}\")\n",
    "        print(f\"Warning: Capability Statement file not found at {capability_statement_file}. Proceeding without it.\")\n",
    "        capability_statement_file = None\n",
    "    \n",
    "    # Get IG name\n",
    "    ig_name = input(\"\\nEnter Implementation Guide name (default 'FHIR Implementation Guide'): \") or \"FHIR Implementation Guide\"\n",
    "    \n",
    "    # Get output directory or use default\n",
    "    output_dir = input(f\"\\nEnter output directory path or accept default (default '{DEFAULT_OUTPUT_DIR}'): \") or str(DEFAULT_OUTPUT_DIR)\n",
    "    output_dir_path = Path(output_dir)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Let user select the API\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice of API to use, based on the printed listing (1-3, default 1): \") or \"1\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"claude\")\n",
    "    \n",
    "    print(f\"\\nProcessing requirements with {api_type.capitalize()}...\")\n",
    "    if capability_statement_file:\n",
    "        print(f\"Including Capability Statement from {capability_statement_file}\")\n",
    "    print(f\"This may take several minutes depending on the number of requirements.\")\n",
    "    \n",
    "    try:\n",
    "        # Process requirements and generate test plan\n",
    "        result = generate_consolidated_test_plan(\n",
    "            api_type=api_type,\n",
    "            requirements_file=requirements_file,\n",
    "            capability_statement_file=capability_statement_file,\n",
    "            ig_name=ig_name,\n",
    "            output_dir=output_dir_path\n",
    "        )\n",
    "        \n",
    "        # Calculate elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "        # Format as hours:minutes:seconds\n",
    "        elapsed_time_formatted = str(timedelta(seconds=int(elapsed_time)))\n",
    "\n",
    "        # Output results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Test plan generation complete!\")\n",
    "        print(f\"Processed {result['requirements_count']} requirements\")\n",
    "        print(f\"Grouped into {result['group_count']} categories\")\n",
    "        print(f\"Consolidated test plan: {result['test_plan_path']}\")\n",
    "        print(f\"Total execution time: {elapsed_time_formatted}\")\n",
    "        print(\"=\"*80)\n",
    "        # Add execution time to result\n",
    "        result[\"execution_time_seconds\"] = elapsed_time\n",
    "        result[\"execution_time_formatted\"] = elapsed_time_formatted\n",
    "\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log for more details.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FHIR IG Test Plan Generator\n",
      "==================================================\n",
      "\n",
      "Most recent files:\n",
      "1. small-plan-net-requirements-v5.md (2025-06-25 13:24)\n",
      "2. small-plan-net-requirements-v4.md (2025-06-25 13:23)\n",
      "3. testable-conformanceverb-plan-net-requirements.md (2025-06-25 13:21)\n",
      "4. small-plan-net-requirements-v3.md (2025-06-16 08:40)\n",
      "5. plan-net-requirements.md (2025-04-29 13:55)\n",
      "6. example_claude_reqs_list_v2_20250416_141601.md (2025-04-23 10:47)\n",
      "\n",
      "Options:\n",
      "- Select a number (1-10) to choose a file\n",
      "- Enter 'all' to see all files\n",
      "- Enter a full path to use a specific file\n",
      "\n",
      "Found capability statement files:\n",
      "1. CapabilityStatement_plan_net.md (2025-05-23 15:40)\n",
      "\n",
      "Options:\n",
      "- Select a number to choose a capability statement file\n",
      "- Press Enter to use the most recent file\n",
      "- Enter 'none' to skip using a capability statement\n",
      "- Enter a full path to use a specific file\n",
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 13:25:26,029 - __main__ - INFO - Starting test plan generation with claude for Plan Net\n",
      "2025-06-25 13:25:26,057 - __main__ - INFO - Parsed 5 requirements from /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/revised_reqs_output/small-plan-net-requirements-v4.md\n",
      "2025-06-25 13:25:26,059 - __main__ - INFO - Parsed capability statement from /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned/CapabilityStatement_plan_net.md\n",
      "2025-06-25 13:25:26,059 - __main__ - INFO - Identifying group for requirement REQ-02 using claude...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing requirements with Claude...\n",
      "Including Capability Statement from /Users/ceadams/Documents/onclaive/onclaive/full-ig/markdown7_cleaned/CapabilityStatement_plan_net.md\n",
      "This may take several minutes depending on the number of requirements.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 13:25:27,564 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:25:28,070 - __main__ - INFO - Identifying group for requirement REQ-03 using claude...\n",
      "2025-06-25 13:25:29,950 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:25:30,456 - __main__ - INFO - Identifying group for requirement REQ-06 using claude...\n",
      "2025-06-25 13:25:32,619 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:25:33,124 - __main__ - INFO - Identifying group for requirement REQ-09 using claude...\n",
      "2025-06-25 13:25:37,243 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:25:37,752 - __main__ - INFO - Identifying group for requirement REQ-10 using claude...\n",
      "2025-06-25 13:25:39,341 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:25:39,843 - __main__ - INFO - Requirements grouped into 5 categories\n",
      "2025-06-25 13:25:39,848 - __main__ - INFO - Group 'Security and Privacy': 1 requirements\n",
      "2025-06-25 13:25:39,849 - __main__ - INFO - Group 'General Capability': 1 requirements\n",
      "2025-06-25 13:25:39,849 - __main__ - INFO - Group 'General Conformance': 1 requirements\n",
      "2025-06-25 13:25:39,849 - __main__ - INFO - Group 'General Processing': 1 requirements\n",
      "2025-06-25 13:25:39,850 - __main__ - INFO - Group 'System-Wide': 1 requirements\n",
      "2025-06-25 13:25:39,850 - __main__ - INFO - Processing requirement for group 'General Capability': REQ-03\n",
      "2025-06-25 13:25:39,851 - __main__ - INFO - Generating test specification for REQ-03 using claude...\n",
      "2025-06-25 13:25:52,716 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:25:52,717 - __main__ - INFO - Processing requirement for group 'General Conformance': REQ-06\n",
      "2025-06-25 13:25:52,717 - __main__ - INFO - Generating test specification for REQ-06 using claude...\n",
      "2025-06-25 13:26:04,905 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:26:04,906 - __main__ - INFO - Processing requirement for group 'General Processing': REQ-09\n",
      "2025-06-25 13:26:04,907 - __main__ - INFO - Generating test specification for REQ-09 using claude...\n",
      "2025-06-25 13:26:19,434 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:26:19,434 - __main__ - INFO - Processing requirement for group 'Security and Privacy': REQ-02\n",
      "2025-06-25 13:26:19,435 - __main__ - INFO - Generating test specification for REQ-02 using claude...\n",
      "2025-06-25 13:26:31,754 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:26:31,756 - __main__ - INFO - Processing requirement for group 'System-Wide': REQ-10\n",
      "2025-06-25 13:26:31,756 - __main__ - INFO - Generating test specification for REQ-10 using claude...\n",
      "2025-06-25 13:26:45,934 - httpx - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "2025-06-25 13:26:45,938 - __main__ - INFO - Consolidated test plan saved to /Users/ceadams/Documents/onclaive/onclaive/test_kit_dev/test_plan_output/claude_test_plan_20250625_132526.md\n",
      "2025-06-25 13:26:45,939 - __main__ - ERROR - Error: type object 'datetime.datetime' has no attribute 'timedelta'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error occurred during processing: type object 'datetime.datetime' has no attribute 'timedelta'\n",
      "Check the log for more details.\n"
     ]
    }
   ],
   "source": [
    "run_test_plan_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

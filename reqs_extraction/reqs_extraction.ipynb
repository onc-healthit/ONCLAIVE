{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FHIR Implementation Guide Requirements Extractor\n",
    "\n",
    "This notebook extracts testable requirements from FHIR Implementation Guides and formats them according to INCOSE Systems Engineering standards.\n",
    "\n",
    "#### Features\n",
    "- Processes markdown files from FHIR Implementation Guides\n",
    "- Extracts clear, testable requirements with proper attribution\n",
    "- Formats requirements in standardized INCOSE format\n",
    "- Handles large documents through chunking\n",
    "\n",
    "#### Usage\n",
    "1. Input markdown directory can be set to `full-ig/markdown7_demo` or `full-ig/markdown7_cleaned` for limited set of 1 or 7 markdown files or to `full-ig/markdown_cleaned` for full set of 300+ markdown files; default is currently `full-ig/demo`\n",
    "2. Run all cells in this notebook\n",
    "3. When prompted, enter input file path, capability statement file path, output directory, and IG name (or accept defaults)\n",
    "4. Select the LLM engine to use\n",
    "5. The script will generate one output file; the default output directory is `reqs_extraction/initial_reqs_output` but another can be specified\n",
    "   - Clean requirements list formatted to INCOSE standards\n",
    "\n",
    "#### Notes:\n",
    "- Supports Claude, Gemini, or GPT-4o\n",
    "- API keys should be in .env file\n",
    "- API configurations are set in llm_utils.py- changes to configurations should be made there\n",
    "- Individual cert setup may need to be modified in `setup_clients()` function in the llm_utils.py file before running this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import os\n",
    "import logging\n",
    "from typing import List, Dict, Union, Optional, Any\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import httpx\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from anthropic import Anthropic, RateLimitError\n",
    "import google.generativeai as gemini\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Current working directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction\n",
      "INFO:root:Project root: /Users/ceadams/Documents/onclaive/onclaive\n",
      "INFO:root:Default markdown directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/demo\n",
      "INFO:root:Default output directory: /Users/ceadams/Documents/onclaive/onclaive/reqs_extraction/initial_reqs_output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the current working directory and set up paths\n",
    "PROJECT_ROOT = Path.cwd().parent  # Go up one level from reqs_extraction to onclaive root\n",
    "DEFAULT_MARKDOWN_DIR = os.path.join(PROJECT_ROOT, 'full-ig', 'demo')\n",
    "DEFAULT_OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'reqs_extraction', 'initial_reqs_output')\n",
    "\n",
    "# Add debug logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.info(f\"Current working directory: {Path.cwd()}\")\n",
    "logging.info(f\"Project root: {PROJECT_ROOT}\")\n",
    "logging.info(f\"Default markdown directory: {DEFAULT_MARKDOWN_DIR}\")\n",
    "logging.info(f\"Default output directory: {DEFAULT_OUTPUT_DIR}\")\n",
    "\n",
    "# Basic setup\n",
    "load_dotenv(os.path.join(PROJECT_ROOT, '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "module_path = os.path.join(PROJECT_ROOT, 'llm_utils.py')\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm_utils\", module_path)\n",
    "llm_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(llm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Prompt environment set up at: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "INFO:root:Using prompts directory: /Users/ceadams/Documents/onclaive/onclaive/prompts\n",
      "INFO:root:Requirements extraction prompt: /Users/ceadams/Documents/onclaive/onclaive/prompts/requirements_extraction.md\n"
     ]
    }
   ],
   "source": [
    "# Import prompt utilities\n",
    "prompt_utils_path = os.path.join(PROJECT_ROOT, 'prompt_utils.py')\n",
    "if os.path.exists(prompt_utils_path):\n",
    "    spec = importlib.util.spec_from_file_location(\"prompt_utils\", prompt_utils_path)\n",
    "    prompt_utils = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(prompt_utils)\n",
    "    \n",
    "    # Setup the prompt environment\n",
    "    prompt_env = prompt_utils.setup_prompt_environment(PROJECT_ROOT)\n",
    "    PROMPT_DIR = prompt_env[\"prompt_dir\"]\n",
    "    REQUIREMENTS_EXTRACTION_PATH = prompt_env[\"requirements_extraction_path\"]\n",
    "    \n",
    "    logging.info(f\"Using prompts directory: {PROMPT_DIR}\")\n",
    "    logging.info(f\"Requirements extraction prompt: {REQUIREMENTS_EXTRACTION_PATH}\")\n",
    "else:\n",
    "    logging.warning(\"prompt_utils.py not found at expected location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPTS = {\n",
    "    \"claude\": \"\"\"You are a seasoned Healthcare Integration Test Engineer with expertise in INCOSE Systems Engineering standards, \n",
    "    analyzing a FHIR Implementation Guide to extract precise testable requirements in INCOSE format.\"\"\",\n",
    "    \"gemini\": \"\"\"You are a Healthcare Integration Test Engineer with expertise in INCOSE Systems Engineering standards, analyzing FHIR \n",
    "    Implementation Guide content to identify and format testable requirements following INCOSE specifications.\"\"\",\n",
    "    \"gpt\": \"\"\"As a Healthcare Integration Test Engineer with INCOSE Systems Engineering expertise, analyze this FHIR \n",
    "    Implementation Guide content to extract specific testable requirements in INCOSE-compliant format.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining and Chunking Markdown Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_markdown_files(markdown_dir):\n",
    "    \"\"\"Debug function to list all markdown files\"\"\"\n",
    "    if not os.path.exists(markdown_dir):\n",
    "        logging.error(f\"Directory does not exist: {markdown_dir}\")\n",
    "        return\n",
    "    \n",
    "    files = [f for f in os.listdir(markdown_dir) if f.endswith('.md')]\n",
    "    logging.info(f\"Found {len(files)} markdown files:\")\n",
    "    for file in files:\n",
    "        logging.info(f\"  - {file}\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optimal_chunk_size(api_type: str, markdown_content: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the optimal chunk size based on API type and content characteristics.\n",
    "    \"\"\"\n",
    "    config = llm_utils.API_CONFIGS[api_type]\n",
    "    \n",
    "    # Base chunk sizes based on API token limits\n",
    "    base_chunk_sizes = {\n",
    "        \"claude\": 8000,  # Claude has higher token limits\n",
    "        \"gemini\": 7000,  # Gemini is also capable of handling larger chunks\n",
    "        \"gpt\": 3000      # GPT-4 with smaller context\n",
    "    }\n",
    "    \n",
    "    # Start with the base size for the API\n",
    "    optimal_size = base_chunk_sizes[api_type]\n",
    "    \n",
    "    # Adjust based on content characteristics\n",
    "    content_length = len(markdown_content)\n",
    "    \n",
    "    # For very small content, don't chunk at all\n",
    "    if content_length <= optimal_size / 2:\n",
    "        return content_length\n",
    "    \n",
    "    # For medium content, use the base size\n",
    "    if content_length <= optimal_size * 1.5:\n",
    "        return optimal_size\n",
    "    \n",
    "    # For larger content, adjust based on complexity \n",
    "    code_blocks = markdown_content.count(\"```\")\n",
    "    tables = markdown_content.count(\"|\")\n",
    "    \n",
    "    # Adjust down if content has complex structures\n",
    "    complexity_factor = 1.0\n",
    "    if code_blocks > 5:\n",
    "        complexity_factor *= 0.9\n",
    "    if tables > 10:\n",
    "        complexity_factor *= 0.9\n",
    "    \n",
    "    # Avoid exceeding API token limits\n",
    "    return min(int(optimal_size * complexity_factor), base_chunk_sizes[api_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markdown Processing Functions\n",
    "def clean_markdown(text: str) -> str:\n",
    "    \"\"\"Clean markdown content\"\"\"\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'<!--.*?-->', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\\\(.)', r'\\1', text)\n",
    "    text = re.sub(r'\\|', ' ', text)\n",
    "    text = re.sub(r'[-\\s]*\\n[-\\s]*', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_markdown_dynamic(content: str, api_type: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split markdown into dynamically sized chunks based on API type and content.\n",
    "    \"\"\"\n",
    "    # If content is very small, don't split it\n",
    "    if len(content) < 1000:\n",
    "        return [content]\n",
    "    \n",
    "    # Calculate optimal chunk size\n",
    "    max_size = calculate_optimal_chunk_size(api_type, content)\n",
    "    \n",
    "    chunks = []\n",
    "    lines = content.split('\\n')\n",
    "    current_chunk = []\n",
    "    current_size = 0\n",
    "    \n",
    "    # Try to split at meaningful boundaries like headers or blank lines\n",
    "    for i, line in enumerate(lines):\n",
    "        line_size = len(line)\n",
    "        \n",
    "        if current_size + line_size > max_size:\n",
    "            # Look back for a good splitting point (blank line or header)\n",
    "            split_index = find_good_split_point(current_chunk)\n",
    "            \n",
    "            if split_index > 0:\n",
    "                # Split at the good point\n",
    "                first_part = current_chunk[:split_index]\n",
    "                second_part = current_chunk[split_index:]\n",
    "                chunks.append('\\n'.join(first_part))\n",
    "                current_chunk = second_part\n",
    "                current_size = sum(len(l) for l in second_part)\n",
    "            else:\n",
    "                # If no good splitting point, use the current chunk\n",
    "                chunks.append('\\n'.join(current_chunk))\n",
    "                current_chunk = []\n",
    "                current_size = 0\n",
    "            \n",
    "            # Add the current line to the new chunk\n",
    "            current_chunk.append(line)\n",
    "            current_size += line_size\n",
    "        else:\n",
    "            current_chunk.append(line)\n",
    "            current_size += line_size\n",
    "    \n",
    "    # Add the last chunk if there's anything left\n",
    "    if current_chunk:\n",
    "        chunks.append('\\n'.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def find_good_split_point(lines: List[str]) -> int:\n",
    "    \"\"\"\n",
    "    Find a good place to split a chunk, preferring blank lines or headers.\n",
    "    \"\"\"\n",
    "    # Go backwards from the end to find a natural splitting point\n",
    "    for i in range(len(lines) - 1, 0, -1):\n",
    "        # Prefer blank lines\n",
    "        if lines[i].strip() == '':\n",
    "            return i + 1\n",
    "        \n",
    "        # Or headers\n",
    "        if lines[i].startswith('#') or lines[i].startswith('==') or lines[i].startswith('--'):\n",
    "            return i\n",
    "    \n",
    "    # If we're more than halfway through, just use the current point\n",
    "    return len(lines) // 2\n",
    "\n",
    "def should_combine_files(files: List[str], markdown_dir: str, api_type: str) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Determine if small files should be combined for processing.\n",
    "    \"\"\"\n",
    "    config = llm_utils.API_CONFIGS[api_type]\n",
    "    file_sizes = {}\n",
    "    \n",
    "    # Get the size of each file\n",
    "    for file in files:\n",
    "        file_path = os.path.join(markdown_dir, file)\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            file_sizes[file] = len(content)\n",
    "    \n",
    "    # Estimate the optimal size based on API\n",
    "    optimal_sizes = {\n",
    "        \"claude\": 12000,\n",
    "        \"gemini\": 10000,\n",
    "        \"gpt\": 6000\n",
    "    }\n",
    "    \n",
    "    optimal_size = optimal_sizes[api_type]\n",
    "    combined_files = []\n",
    "    current_group = []\n",
    "    current_size = 0\n",
    "    \n",
    "    # Sort files by size (ascending) to try combining smaller files first\n",
    "    sorted_files = sorted(files, key=lambda f: file_sizes[f])\n",
    "    \n",
    "    for file in sorted_files:\n",
    "        size = file_sizes[file]\n",
    "        \n",
    "        # If this file is already big, process it individually\n",
    "        if size > optimal_size * 0.8:\n",
    "            if current_group:\n",
    "                combined_files.append(current_group)\n",
    "                current_group = []\n",
    "                current_size = 0\n",
    "            combined_files.append([file])\n",
    "            continue\n",
    "        \n",
    "        # If adding this file would exceed optimal size, start a new group\n",
    "        if current_size + size > optimal_size:\n",
    "            if current_group:\n",
    "                combined_files.append(current_group)\n",
    "            current_group = [file]\n",
    "            current_size = size\n",
    "        else:\n",
    "            current_group.append(file)\n",
    "            current_size += size\n",
    "    \n",
    "    # Add the last group if there's anything left\n",
    "    if current_group:\n",
    "        combined_files.append(current_group)\n",
    "    \n",
    "    return combined_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate Limiting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a rate limiter\n",
    "rate_limiter = llm_utils.create_rate_limiter()\n",
    "\n",
    "def create_rate_limit_func(api_type_name):\n",
    "    \"\"\"Create a rate limiting function for a specific API\"\"\"\n",
    "    return llm_utils.create_rate_limit_function(rate_limiter, api_type_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_incose_requirements_extraction_prompt(content: str, chunk_index: int, total_chunks: int) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for extracting requirements in INCOSE format using external prompt file\n",
    "    \n",
    "    Args:\n",
    "        content: The content to analyze\n",
    "        chunk_index: Index of this chunk in the total content\n",
    "        total_chunks: Total number of chunks being processed\n",
    "        \n",
    "    Returns:\n",
    "        str: The prompt for the LLM loaded from external file\n",
    "    \"\"\"\n",
    "    return prompt_utils.load_prompt(\n",
    "        REQUIREMENTS_EXTRACTION_PATH,\n",
    "        content=content,\n",
    "        chunk_index=chunk_index,\n",
    "        total_chunks=total_chunks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for API Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content_for_api(content: Union[str, dict, list], api_type: str, chunk_index: int, total_chunks: int) -> Union[str, List[dict], dict]:\n",
    "    \"\"\"Format content appropriately for each API\"\"\"\n",
    "    base_prompt = create_incose_requirements_extraction_prompt(content, chunk_index, total_chunks)\n",
    "    \n",
    "    if api_type == \"claude\":\n",
    "        return [{\n",
    "            \"type\": \"text\",\n",
    "            \"text\": base_prompt\n",
    "        }]\n",
    "    elif api_type == \"gemini\":\n",
    "        return [{  \n",
    "            \"parts\": [{\n",
    "                \"text\": base_prompt\n",
    "            }]\n",
    "        }]\n",
    "    return base_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(client, api_type: str, content: str, rate_limit_func, chunk_index: int, total_chunks: int) -> str:\n",
    "    \"\"\"Make rate-limited API request with retries\"\"\"\n",
    "    # Ensure we have a callable rate limiter function\n",
    "    if not callable(rate_limit_func):\n",
    "        rate_limit_func = create_rate_limit_func(api_type)\n",
    "    \n",
    "    formatted_content = format_content_for_api(content, api_type, chunk_index, total_chunks)\n",
    "    \n",
    "    # For Claude and others that need special formatting\n",
    "    if api_type == \"claude\":\n",
    "        prompt_text = formatted_content[0][\"text\"]\n",
    "    elif api_type == \"gemini\":\n",
    "        prompt_text = formatted_content[0][\"parts\"][0][\"text\"]\n",
    "    else:\n",
    "        prompt_text = formatted_content\n",
    "        \n",
    "    return llm_utils.make_llm_request(\n",
    "        client=client,\n",
    "        api_type=api_type,\n",
    "        prompt=prompt_text,\n",
    "        system_prompt=SYSTEM_PROMPTS[api_type],\n",
    "        rate_limit_func=rate_limit_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content_batch(api_type: str, contents: List[str], config: dict, client) -> List[str]:\n",
    "    \"\"\"Process a batch of content with rate limiting\"\"\"\n",
    "    results = []\n",
    "    total_chunks = len(contents)\n",
    "    \n",
    "    # Create a rate limit function for this API\n",
    "    rate_limit_func = create_rate_limit_func(api_type)\n",
    "    \n",
    "    for chunk_idx, content in enumerate(contents, 1):\n",
    "        result = make_api_request(client, api_type, content, rate_limit_func, \n",
    "                                chunk_idx, total_chunks)\n",
    "        results.append(result)\n",
    "        time.sleep(config[\"delay_between_chunks\"])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_markdown_content_for_incose_srs(api_type: str, markdown_dir: str, \n",
    "                                           output_directory: str = None) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process markdown content and generate INCOSE SRS document directly from LLM outputs.\n",
    "    \n",
    "    Args:\n",
    "        api_type: The API to use for processing\n",
    "        markdown_dir: Directory containing markdown files\n",
    "        output_directory: Directory to save output files (optional)\n",
    "        \n",
    "    Returns:\n",
    "        Dict containing processing results and SRS document\n",
    "    \"\"\"\n",
    "    logging.info(f\"Starting processing with {api_type} on directory: {markdown_dir}\")\n",
    "    \n",
    "    # Use default output directory if none provided\n",
    "    if output_directory is None:\n",
    "        output_directory = os.path.join(PROJECT_ROOT, 'reqs_extraction', 'initial_reqs_output')\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # List files before processing\n",
    "    markdown_files = list_markdown_files(markdown_dir)\n",
    "    if not markdown_files:\n",
    "        logging.error(\"No markdown files found to process\")\n",
    "        return {\"processed_files\": [], \"srs_document\": \"\", \"output_file\": None}\n",
    "    \n",
    "    # Initialize API clients and rate limiters - using utilities\n",
    "    clients = llm_utils.setup_clients()\n",
    "    client = clients[api_type]\n",
    "    config = llm_utils.API_CONFIGS[api_type]\n",
    "    rate_limiter = llm_utils.create_rate_limiter()\n",
    "    \n",
    "    # Create a rate limit function for this specific API\n",
    "    rate_limit_func = llm_utils.create_rate_limit_function(rate_limiter, api_type)\n",
    "    \n",
    "    try:\n",
    "        all_incose_requirements = []\n",
    "        processed_files = []\n",
    "        \n",
    "        # Group files for potential combination\n",
    "        file_groups = should_combine_files(markdown_files, markdown_dir, api_type)\n",
    "        logging.info(f\"Organized {len(markdown_files)} files into {len(file_groups)} processing groups\")\n",
    "        \n",
    "        for group in file_groups:\n",
    "            # For a single file\n",
    "            if len(group) == 1:\n",
    "                file_path = os.path.join(markdown_dir, group[0])\n",
    "                logging.info(f\"Processing single file: {group[0]}\")\n",
    "                \n",
    "                with open(file_path, 'r') as f:\n",
    "                    content = clean_markdown(f.read())\n",
    "                \n",
    "                # Use dynamic chunk sizing\n",
    "                chunks = split_markdown_dynamic(content, api_type)\n",
    "                logging.info(f\"Split {group[0]} into {len(chunks)} chunks using dynamic sizing\")\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks, 1):\n",
    "                    logging.info(f\"Processing chunk {chunk_idx}/{len(chunks)} of {group[0]}\")\n",
    "                    \n",
    "                    # Format the content for the API\n",
    "                    formatted_content = format_content_for_api(content, api_type, chunk_idx, len(chunks))\n",
    "                    \n",
    "                    # Extract proper text based on API type\n",
    "                    if api_type == \"claude\":\n",
    "                        prompt_text = formatted_content[0][\"text\"]\n",
    "                    elif api_type == \"gemini\":\n",
    "                        prompt_text = formatted_content[0][\"parts\"][0][\"text\"]\n",
    "                    else:\n",
    "                        prompt_text = formatted_content\n",
    "                    \n",
    "                    # Use the utility function for API request\n",
    "                    response = llm_utils.make_llm_request(\n",
    "                        client=client,\n",
    "                        api_type=api_type,\n",
    "                        prompt=prompt_text,\n",
    "                        system_prompt=SYSTEM_PROMPTS[api_type],\n",
    "                        rate_limit_func=rate_limit_func\n",
    "                    )\n",
    "                    \n",
    "                    all_incose_requirements.append(response)\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "                \n",
    "                processed_files.append(group[0])\n",
    "                \n",
    "            # For multiple combined files\n",
    "            else:\n",
    "                logging.info(f\"Processing combined group of {len(group)} files\")\n",
    "                combined_content = []\n",
    "                \n",
    "                # Prepare combined content with clear file boundaries\n",
    "                for file in group:\n",
    "                    file_path = os.path.join(markdown_dir, file)\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        file_content = clean_markdown(f.read())\n",
    "                        combined_content.append(f\"## FILE: {file}\\n\\n{file_content}\\n\\n\")\n",
    "                \n",
    "                combined_text = \"\".join(combined_content)\n",
    "                chunks = split_markdown_dynamic(combined_text, api_type)\n",
    "                logging.info(f\"Split combined content into {len(chunks)} chunks\")\n",
    "                \n",
    "                for chunk_idx, chunk in enumerate(chunks, 1):\n",
    "                    logging.info(f\"Processing chunk {chunk_idx}/{len(chunks)} of combined files\")\n",
    "                    \n",
    "                    # Format the content for the API\n",
    "                    formatted_content = format_content_for_api(chunk, api_type, chunk_idx, len(chunks))\n",
    "                    \n",
    "                    # Extract proper text based on API type\n",
    "                    if api_type == \"claude\":\n",
    "                        prompt_text = formatted_content[0][\"text\"]\n",
    "                    elif api_type == \"gemini\":\n",
    "                        prompt_text = formatted_content[0][\"parts\"][0][\"text\"]\n",
    "                    else:\n",
    "                        prompt_text = formatted_content\n",
    "                    \n",
    "                    # Use the utility function for API request\n",
    "                    response = llm_utils.make_llm_request(\n",
    "                        client=client,\n",
    "                        api_type=api_type,\n",
    "                        prompt=prompt_text,\n",
    "                        system_prompt=SYSTEM_PROMPTS[api_type],\n",
    "                        rate_limit_func=rate_limit_func\n",
    "                    )\n",
    "                    \n",
    "                    all_incose_requirements.append(response)\n",
    "                    time.sleep(config[\"delay_between_chunks\"])\n",
    "                \n",
    "                processed_files.extend(group)\n",
    "            \n",
    "            # Add delay between file groups\n",
    "            time.sleep(config[\"delay_between_batches\"])\n",
    "        \n",
    "        # Combine all requirements into a full INCOSE SRS document\n",
    "        srs_document = \"\"\n",
    "        for req_section in all_incose_requirements:\n",
    "            # Skip the intro text if it's present to avoid duplication\n",
    "            # Look for the first requirement section\n",
    "            if \"## REQ-\" in req_section:\n",
    "                # Find the index of the first requirement\n",
    "                req_start_idx = req_section.find(\"## REQ-\")\n",
    "                if req_start_idx > 0:\n",
    "                    # Only add the requirements part, not any introductory text\n",
    "                    srs_document += req_section[req_start_idx:]\n",
    "                else:\n",
    "                    srs_document += req_section\n",
    "            else:\n",
    "                # Add any content that doesn't contain requirements\n",
    "                # (this could be informational text related to requirements)\n",
    "                srs_document += req_section\n",
    "        \n",
    "        # Save INCOSE SRS document to markdown file\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        srs_output_file = os.path.join(output_directory, f\"{api_type}_reqs_list_v1_{timestamp}.md\")\n",
    "        \n",
    "        with open(srs_output_file, 'w') as f:\n",
    "            f.write(srs_document)\n",
    "        \n",
    "        logging.info(f\"Completed processing {len(processed_files)} files\")\n",
    "        logging.info(f\"Generated requirements document saved to {srs_output_file}\")\n",
    "        \n",
    "        return {\n",
    "            \"processed_files\": processed_files,\n",
    "            \"srs_document\": srs_document,\n",
    "            \"output_file\": srs_output_file\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing content: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_requirements_extractor():\n",
    "    \"\"\"\n",
    "    Main function to run the requirements extraction process.\n",
    "    Handles user input, processes markdown files, and generates INCOSE-formatted requirements.\n",
    "    \"\"\"\n",
    "    # Get markdown directory from user input or use default\n",
    "    markdown_dir = input(f\"Enter directory path for IG markdown files or accept default (default '{DEFAULT_MARKDOWN_DIR}'): \") or DEFAULT_MARKDOWN_DIR\n",
    "    output_directory = input(f\"Enter output directory path or accept default (default '{DEFAULT_OUTPUT_DIR}'): \") or DEFAULT_OUTPUT_DIR\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Verify the markdown directory exists\n",
    "    if os.path.exists(markdown_dir):\n",
    "        logging.info(f\"Found markdown directory at {markdown_dir}\")\n",
    "        markdown_files = [f for f in os.listdir(markdown_dir) if f.endswith('.md')]\n",
    "        logging.info(f\"Found {len(markdown_files)} markdown files\")\n",
    "    else:\n",
    "        logging.error(f\"Markdown directory not found at {markdown_dir}\")\n",
    "        print(f\"Error: Directory not found at {markdown_dir}\")\n",
    "        return  # Exit the function if directory doesn't exist\n",
    "\n",
    "    # Choose which API to use\n",
    "    print(\"\\nSelect the API to use:\")\n",
    "    print(\"1. Claude\")\n",
    "    print(\"2. Gemini\")\n",
    "    print(\"3. GPT-4\")\n",
    "    api_choice = input(\"Enter your choice of API to use, based on the printed listing (1-3, default 2): \") or \"2\"\n",
    "    \n",
    "    api_mapping = {\n",
    "        \"1\": \"claude\",\n",
    "        \"2\": \"gemini\",\n",
    "        \"3\": \"gpt\"\n",
    "    }\n",
    "    \n",
    "    api_type = api_mapping.get(api_choice, \"gemini\")\n",
    "    rate_limit_func = create_rate_limit_func(api_type)\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Processing with {api_type}...\")\n",
    "        print(f\"\\nProcessing Implementation Guide with {api_type.capitalize()}...\")\n",
    "        print(f\"This may take several minutes depending on the size of the Implementation Guide.\")\n",
    "        \n",
    "        # Process the markdown files and generate direct INCOSE SRS document\n",
    "        api_results = process_markdown_content_for_incose_srs(\n",
    "            api_type=api_type, \n",
    "            markdown_dir=markdown_dir,\n",
    "            output_directory=output_directory\n",
    "        )\n",
    "        \n",
    "        # Output the results to the user\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"Processing complete!\")\n",
    "        print(f\"Generated requirements document: {api_results['output_file']}\")\n",
    "        print(f\"Processed {len(api_results['processed_files'])} files\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {api_type}: {str(e)}\")\n",
    "        print(f\"\\nError occurred during processing: {str(e)}\")\n",
    "        print(\"Check the log file for more details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found markdown directory at /Users/ceadams/Documents/onclaive/onclaive/full-ig/demo\n",
      "INFO:root:Found 1 markdown files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Select the API to use:\n",
      "1. Claude\n",
      "2. Gemini\n",
      "3. GPT-4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Processing with claude...\n",
      "INFO:root:Starting processing with claude on directory: /Users/ceadams/Documents/onclaive/onclaive/full-ig/demo\n",
      "INFO:root:Found 1 markdown files:\n",
      "INFO:root:  - implementation.md\n",
      "INFO:root:Organized 1 files into 1 processing groups\n",
      "INFO:root:Processing single file: implementation.md\n",
      "INFO:root:Split implementation.md into 2 chunks using dynamic sizing\n",
      "INFO:root:Processing chunk 1/2 of implementation.md\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Implementation Guide with Claude...\n",
      "This may take several minutes depending on the size of the Implementation Guide.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the requirements extractor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mrun_requirements_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrun_requirements_extractor\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis may take several minutes depending on the size of the Implementation Guide.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Process the markdown files and generate direct INCOSE SRS document\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m api_results = \u001b[43mprocess_markdown_content_for_incose_srs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmarkdown_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmarkdown_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_directory\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Output the results to the user\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mprocess_markdown_content_for_incose_srs\u001b[39m\u001b[34m(api_type, markdown_dir, output_directory)\u001b[39m\n\u001b[32m     71\u001b[39m     prompt_text = formatted_content\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Use the utility function for API request\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m response = \u001b[43mllm_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_llm_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSYSTEM_PROMPTS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mapi_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrate_limit_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrate_limit_func\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m all_incose_requirements.append(response)\n\u001b[32m     83\u001b[39m time.sleep(config[\u001b[33m\"\u001b[39m\u001b[33mdelay_between_chunks\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:336\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    334\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    335\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:475\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    473\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:376\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    374\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:398\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    397\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    399\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/tenacity/__init__.py:478\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    480\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/llm_utils.py:182\u001b[39m, in \u001b[36mmake_llm_request\u001b[39m\u001b[34m(client, api_type, prompt, system_prompt, rate_limit_func)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m api_type == \u001b[33m\"\u001b[39m\u001b[33mclaude\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response.content[\u001b[32m0\u001b[39m].text\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m api_type == \u001b[33m\"\u001b[39m\u001b[33mgemini\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:274\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/resources/messages.py:888\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    882\u001b[39m     warnings.warn(\n\u001b[32m    883\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    884\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    885\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    886\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m888\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1277\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1264\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1265\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1273\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1274\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1275\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1276\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:954\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    952\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:990\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m    987\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, request.method, request.url)\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    996\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:926\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    924\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:954\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    951\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    960\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:991\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    989\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    993\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_client.py:1027\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1022\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1023\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1024\u001b[39m     )\n\u001b[32m   1026\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1031\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:236\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    223\u001b[39m req = httpcore.Request(\n\u001b[32m    224\u001b[39m     method=request.method,\n\u001b[32m    225\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m     extensions=request.extensions,\n\u001b[32m    234\u001b[39m )\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    241\u001b[39m     status_code=resp.status,\n\u001b[32m    242\u001b[39m     headers=resp.headers,\n\u001b[32m    243\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    244\u001b[39m     extensions=resp.extensions,\n\u001b[32m    245\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/onclaive/onclaive/.venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the requirements extractor\n",
    "run_requirements_extractor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

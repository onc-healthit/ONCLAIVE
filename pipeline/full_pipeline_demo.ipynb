{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e3a524",
   "metadata": {},
   "source": [
    "## FHIR Implementation Guide Testing Pipeline\n",
    "This notebook provides a comprehensive pipeline for automatically extracting requirements from FHIR Implementation Guides and generating executable test suites. The pipeline transforms Implementation Guide (IG) documentation into structured test code that can validate FHIR server implementations.\n",
    "\n",
    "#### Overview\n",
    "This automated pipeline takes FHIR Implementation Guide documentation and produces comprehensive test suites through several integrated stages:\n",
    "\n",
    "- Implementation Guide Preparation: Convert and clean IG HTML documentation to markdown format\n",
    "- Requirements Extraction: Use AI to identify and extract testable requirements from the IG\n",
    "- Requirements Refinement: Consolidate and refine the extracted requirements\n",
    "- Requirements Downselection: Combine multiple requirement sets and remove duplicates\n",
    "- Test Plan Generation: Convert requirements into detailed test specifications\n",
    "- Test Kit Generation: Generate executable Inferno test code\n",
    "\n",
    "#### Running this Notebook\n",
    "The notebook is structured to run each stage sequentially. You can either:\n",
    "\n",
    "- Run the complete pipeline: Execute all cells to process a complete IG\n",
    "- Run individual stages: Execute specific sections as needed\n",
    "\n",
    "Inputs and output directories can be customized for each step. The pipeline automatically saves intermediate outputs in checkpoint directories for review and iteration.\n",
    "\n",
    "#### Output Structure\n",
    "The pipeline generates organized outputs in checkpoint directories:\n",
    "\n",
    "checkpoints/\n",
    "\n",
    "├── markdown1/          # Converted markdown files\n",
    "\n",
    "├── markdown2/          # Cleaned markdown files  \n",
    "\n",
    "├── requirements_extraction/   # Initial AI-extracted requirements\n",
    "\n",
    "├── revised_reqs_extraction/  # Refined requirements lists\n",
    "\n",
    "├── requirements_downselect/  # Final consolidated requirements\n",
    "\n",
    "├── testplan_generation/     # Detailed test specifications\n",
    "\n",
    "└── testkit_generation/      # Executable Inferno test suites\n",
    "\n",
    "Each stage preserves its outputs, allowing for iteration, review, and alternative processing paths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d267b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de19ee6",
   "metadata": {},
   "source": [
    "#### Importing required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc5ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import json\n",
    "import llm_utils\n",
    "import importlib\n",
    "import tiktoken\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7a6cc",
   "metadata": {},
   "source": [
    "## Initializing LLM Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(llm_utils)\n",
    "llm_clients = llm_utils.LLMApiClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_clients.clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d80d1",
   "metadata": {},
   "source": [
    "## Implementation Guide Preparation\n",
    "\n",
    "### Stage 1: Text Extraction and Cleaning\n",
    "- Converts HTML IG files to markdown format\n",
    "- Cleans unnecessary content (navigation, headers, formatting artifacts)\n",
    "- Prepares clean, structured text for AI processing\n",
    "\n",
    "Inputs: HTML files from FHIR IG downloads\n",
    "\n",
    "Outputs: Clean markdown files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08717e1c",
   "metadata": {},
   "source": [
    "#### 1a) HTML to Markdown Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff8f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html_narrative_extractor_01 #import html extractor module\n",
    "\n",
    "# Process directory with default settings\n",
    "result = html_narrative_extractor_01.convert_local_html_to_markdown(\n",
    "    input_dir=\"../us-core/test_set\", #input directory of downloaded IG HTML files\n",
    "    output_dir=\"checkpoints/demo/markdown1/\" #output directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0108ece",
   "metadata": {},
   "source": [
    "#### 1b) Markdown Post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown_cleaner_02 #import markdown cleaner module\n",
    "markdown_cleaner_02.process_directory(\n",
    "    input_dir=\"checkpoints/demo/markdown1\", #input directory of IG markdown files\n",
    "    output_dir=\"checkpoints/demo/markdown2/\") #output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3fa2d1",
   "metadata": {},
   "source": [
    "## Stage 2: Requirements Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0566ab",
   "metadata": {},
   "source": [
    "### 2a) Prompt-based Requirement Extraction\n",
    "LLM Requirements Identification\n",
    "- Processes markdown files using LLM to extract clear, testable requirements\n",
    "- Formats requirements according to set standards, following INCOSE guidance\n",
    "- Generates structured requirements with IDs, descriptions, actors, and conformance levels\n",
    "- Handles large documents through chunking\n",
    "- Provides source tracking\n",
    "\n",
    "Inputs: Cleaned IG markdown files\n",
    "\n",
    "Outputs: Structured requirements list as markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reqs_extraction_03 #import LLM requirements extraction module\n",
    "importlib.reload(reqs_extraction_03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31999d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reqs_extraction_03.run_requirements_extractor(\n",
    "    markdown_dir='checkpoints/demo/condition_profile', #input directory of markdown files\n",
    "    output_dir='checkpoints/demo/requirements_extraction/', #output directory\n",
    "    api_type= 'claude', #set API type\n",
    "    client_instance=llm_clients) #initialize llm clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47056998",
   "metadata": {},
   "source": [
    "### 2b) Requirements Refinement\n",
    "LLM-Based Requirements Review & Consolidation\n",
    "- Filters and identifies only testable requirements from raw extractions\n",
    "- Consolidates duplicate requirements and merges related ones\n",
    "- Applies consistent formatting and structure\n",
    "- Removes non-testable assertions and narrative content\n",
    "\n",
    "Inputs: Raw requirements from extraction stage in markdown format\n",
    "\n",
    "Outputs: Refined requirements list in markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547fa2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requirements refinement script as module\n",
    "import reqs_reviewer_04\n",
    "importlib.reload(reqs_reviewer_04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53363a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = reqs_reviewer_04.run_batch_requirements_refinement(\n",
    "    input_file=\"checkpoints/demo/requirements_extraction/reqs_list_v1.md\", #input requirements list markdown file\n",
    "    output_dir=\"checkpoints/demo/requirements_revision/\", #output directory   \n",
    "    client_instance=llm_clients,  #initialize llm clients\n",
    "    batch_size=25,  #set batch size\n",
    "    api_type=\"claude\"  #set API type\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ccc40",
   "metadata": {},
   "source": [
    "### 2c) Requirements Downselection\n",
    "- Combines multiple requirements lists from different extraction runs\n",
    "- Uses semantic similarity analysis to identify and remove duplicates\n",
    "- Creates a deduplicated final requirements set\n",
    "\n",
    "Inputs: Multiple refined requirements files in markdown or JSON format\n",
    "Outputs: Final consolidated requirements in markdown or JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312c3c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import reqs_downselect_05\n",
    "importlib.reload(reqs_downselect_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4bb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_files_list=reqs_downselect_05.get_md_files_from_directory(\"checkpoints/demo/requirements_revision/\")\n",
    "\n",
    "reqs_downselect_05.full_pass(\n",
    "    md_files=md_files_list,\n",
    "    output_dir=\"checkpoints/demo/requirements_downselect\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a69ed",
   "metadata": {},
   "source": [
    "## Stage 3: Test Plan Generation\n",
    "- Transforms requirements into detailed test specifications\n",
    "- Analyzes IG capability statements for context\n",
    "- Generates implementation strategies with specific FHIR operations\n",
    "- Creates structured test plans with validation criteria\n",
    "\n",
    "Inputs: Refined requirements and IG capability statements in markdown format\n",
    "\n",
    "Outputs: Detailed test plan in markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179dba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "llm_clients.logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf1f5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set logging level to reduce noise\n",
    "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"backoff\").setLevel(logging.ERROR)\n",
    "\n",
    "import test_plan_06 #import test plan generation script as module\n",
    "importlib.reload(test_plan_06)\n",
    "\n",
    "#clearing any existing capability statements from vector database\n",
    "test_plan_06.clear_capability_collection(\"capability_statements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_plan_06.generate_consolidated_test_plan(\n",
    "    client_instance=llm_clients, \n",
    "    api_type='claude',\n",
    "    requirements_file=\"checkpoints/demo/requirements_downselect/consolidated_reqs.md\", #input requirements list markdown file\n",
    "    capability_statement_file=\"checkpoints/demo/markdown2/CapabilityStatement-us-core-server.md\", \n",
    "    ig_name=\"US Core IG\", \n",
    "    output_dir='checkpoints/demo/testplan_generation/', \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080ace6",
   "metadata": {},
   "source": [
    "## Stage 4: Test Kit Generation\n",
    "- Converts test specifications into executable Inferno Ruby tests\n",
    "- Generates complete test suites with proper file organization\n",
    "- Creates modular test structures following Inferno framework patterns\n",
    "- Includes validation and alignment checking\n",
    "\n",
    "Inputs: Test plan specification in markdown format\n",
    "\n",
    "Outputs: Complete Inferno test kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef90585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import test_kit_07\n",
    "importlib.reload(test_kit_07)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771131c2",
   "metadata": {},
   "source": [
    "Without LLM Self Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a684acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster generation- no LLM self evaluation\n",
    "test_kit_07.generate_inferno_test_kit(\n",
    "    client_instance=llm_clients, #initialize llm clients\n",
    "    api_type='claude',  #set API\n",
    "    test_plan_file='checkpoints/demo/testplan_generation/test_plan.md',  #input test plan file\n",
    "    ig_name='US Core',\n",
    "    output_dir='checkpoints/demo/testkit_generation/',\n",
    "    enable_validation=False  #disable LLM self evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d11cfdf",
   "metadata": {},
   "source": [
    "With LLM Self Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5d430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thorough generation- with LLM self evaluation\n",
    "test_kit_07.generate_inferno_test_kit(\n",
    "    client_instance=llm_clients, #initialize llm clients\n",
    "    api_type='claude',  #set API\n",
    "    test_plan_file='checkpoints/demo/testplan_generation/test_plan.md',  #input test plan file\n",
    "    ig_name='US Core',\n",
    "    output_dir='checkpoints/demo/testkit_generation/',\n",
    "    enable_validation=True  #enable LLM self evaluation\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

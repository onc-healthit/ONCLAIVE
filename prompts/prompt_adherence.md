I need you to conduct a comprehensive analysis of how well each LLM followed the original test plan generation prompt. Your task is to evaluate each test plan individually against the specific instructions in the original prompt.

## ORIGINAL TEST PLAN GENERATION PROMPT:
{original_prompt}

## INPUT REQUIREMENTS PROVIDED TO LLMs:
{requirements}

## INPUT CAPABILITY STATEMENT PROVIDED TO LLMs:
{capability_statement}

## GENERATED TEST PLANS TO ANALYZE:
{test_plans_section}

## YOUR TASK: INDIVIDUAL TEST PLAN EVALUATION

You must evaluate each test plan separately against the specific instructions in the original prompt. For each test plan, create a detailed scoring table that examines instruction adherence and provides exact quotes as evidence.

For each test plan, create this analysis structure:

---

## EVALUATION: [Test Plan Name] (Generated by [LLM])

### INSTRUCTION ADHERENCE SCORING TABLE

| Evaluation Category | Score | Evidence from Test Plan |
|-------------------|-------|-------------------------|
| **1. Requirement ID Section** | _/5 | [exact quote or "Not found"] |
| **2. Requirement Analysis Structure** | _/5 | [exact quote or "Not found"] |
| **3. Testability Classification** | _/5 | [exact quote or "Not found"] |
| **4. Complexity Assessment** | _/5 | [exact quote or "Not found"] |
| **5. Prerequisites Definition** | _/5 | [exact quote or "Not found"] |
| **6. Test Implementation Strategy** | _/5 | [exact quote or "Not found"] |
| **7. Required Inputs/Outputs** | _/5 | [exact quote or "Not found"] |
| **8. Required FHIR Operations** | _/5 | [exact quote or "Not found"] |
| **9. Validation Criteria** | _/5 | [exact quote or "Not found"] |
| **10. Markdown Formatting** | _/5 | [description of formatting quality] |
| **11. Interoperability Focus** | _/5 | [exact quote or "Not found"] |
| **12. Bring Your Own Data Approach** | _/5 | [exact quote or "Not found"] |
| **13. No Fixtures Configuration** | _/5 | [exact quote showing violation or "Compliant - no fixtures found"] |
| **14. Capability Statement Integration** | _/5 | [exact quote or "Not found"] |
| **15. Appropriate Abstraction Level** | _/5 | [exact quote or "Not found"] |
| **TOTAL SCORE** | **___/75** | |
| **FINAL GRADE** | **Letter: ___** | |

### DETAILED ANALYSIS FOR THIS TEST PLAN

#### STRENGTHS
- [List 3-5 areas where this test plan followed instructions well]
- [Include specific evidence/quotes for each strength]

#### WEAKNESSES  
- [List 3-5 areas where this test plan failed to follow instructions]
- [Include specific evidence/quotes for each weakness]

#### CRITICAL INSTRUCTION VIOLATIONS
- [Identify the most serious failures to follow the original prompt]
- [Provide exact evidence of these violations]

### OVERALL ASSESSMENT
[2-3 sentence summary of how well this LLM followed the original instructions]

---

[REPEAT THIS ENTIRE STRUCTURE FOR EACH TEST PLAN]

---

## COMPARATIVE SUMMARY

After evaluating all test plans individually, provide this summary:

### FINAL RANKINGS

| Rank | LLM | Test Plan | Score | Grade | Key Strength | Major Weakness |
|------|-----|-----------|-------|-------|--------------|----------------|
| 1 | [LLM] | [filename] | _/75 | [Letter] | [strength] | [weakness] |
| 2 | [LLM] | [filename] | _/75 | [Letter] | [strength] | [weakness] |
| 3 | [LLM] | [filename] | _/75 | [Letter] | [strength] | [weakness] |

### KEY FINDINGS
- **Best instruction follower:** [LLM and specific reasons]
- **Most problematic instruction:** [Which instruction was hardest for all LLMs to follow]
- **Common patterns:** [What patterns emerged across all LLMs]
- **Surprising insights:** [Any unexpected findings about instruction following]

## EVALUATION CRITERIA

### ENHANCED 5-POINT SCORING RUBRIC WITH STRICT STANDARDS

**5/5 - PERFECT ADHERENCE (Exceptional - Reserved for truly flawless execution)**
- Follows instruction exactly as written with no deviations
- Uses precise terminology specified in the prompt
- Demonstrates deep understanding of the instruction's intent
- Goes above and beyond basic compliance

**4/5 - STRONG ADHERENCE (Good - Minor imperfections)**
- Follows instruction well with only very minor deviations
- Uses mostly correct terminology with slight variations
- Shows clear understanding of instruction intent
- Meets all essential requirements

**3/5 - MODERATE ADHERENCE (Satisfactory - Notable issues)**
- Follows general intent but with clear deviations from specifics
- Uses similar but not exact terminology as specified
- Shows partial understanding of instruction
- Meets most requirements but misses some key elements

**2/5 - POOR ADHERENCE (Needs Improvement - Major problems)**
- Attempts to follow instruction but with significant deviations
- Uses different terminology or approach than specified
- Shows limited understanding of instruction intent
- Misses several important requirements

**1/5 - MINIMAL ADHERENCE (Unsatisfactory - Critical failures)**
- Barely attempts to follow the instruction
- Uses completely different approach than specified
- Shows misunderstanding of instruction intent
- Fails to meet most basic requirements

### STRICT INSTRUCTION MAPPING WITH DETAILED REQUIREMENTS

**1. Requirement ID Section**
- **Perfect (5)**: Exactly "1. Requirement ID" or "## 1. Requirement ID" with clear identifier
- **Good (4)**: Similar section with requirement ID but slightly different formatting
- **Moderate (3)**: Has requirement identification but not in specified format
- **Poor (2)**: Mentions requirement but unclear or buried in text
- **Minimal (1)**: No clear requirement identification

**2. Requirement Analysis Structure**
- **Perfect (5)**: Exactly "2. Requirement Analysis:" with all three subsections (Testability, Complexity, Prerequisites)
- **Good (4)**: Has requirement analysis section with most subsections present
- **Moderate (3)**: Has analysis section but missing one subsection or different organization
- **Poor (2)**: Has some analysis but poor organization or missing multiple elements
- **Minimal (1)**: No clear requirement analysis structure

**3. Testability Classification**
- **Perfect (5)**: Uses EXACTLY "automatically testable, an attestation, or not testable due to being too vague or covered by the validator"
- **Good (4)**: Uses the three categories but with slight wording variations
- **Moderate (3)**: Uses similar classification system but different categories
- **Poor (2)**: Attempts classification but uses completely different system
- **Minimal (1)**: No clear testability classification

**4. Complexity Assessment**
- **Perfect (5)**: Uses EXACTLY "Simple, Moderate, or Complex" - no other terms
- **Good (4)**: Uses these terms but with additional descriptors
- **Moderate (3)**: Uses similar complexity levels (Easy/Medium/Hard, Low/Medium/High)
- **Poor (2)**: Uses different complexity assessment approach
- **Minimal (1)**: No complexity assessment provided

**5. Prerequisites Definition**
- **Perfect (5)**: Clear "Prerequisites:" section with system configurations, data, and setup details
- **Good (4)**: Has prerequisites section with most required elements
- **Moderate (3)**: Has some prerequisite information but incomplete or poorly organized
- **Poor (2)**: Minimal prerequisite information
- **Minimal (1)**: No prerequisites section or information

**6. Test Implementation Strategy**
- **Perfect (5)**: Exactly "3. Test Implementation Strategy:" with proper subsections
- **Good (4)**: Has implementation strategy section with most required elements
- **Moderate (3)**: Has strategy section but different organization or missing elements
- **Poor (2)**: Limited strategy information
- **Minimal (1)**: No clear implementation strategy section

**7. Required Inputs/Outputs**
- **Perfect (5)**: Clearly specifies BOTH required inputs AND expected outputs
- **Good (4)**: Specifies inputs and outputs but with minor gaps
- **Moderate (3)**: Specifies mostly inputs OR outputs but not both clearly
- **Poor (2)**: Limited input/output specification
- **Minimal (1)**: No clear input/output requirements

**8. Required FHIR Operations**
- **Perfect (5)**: Lists specific operations AND explicitly references Capability Statement support
- **Good (4)**: Lists operations with some capability statement consideration
- **Moderate (3)**: Lists operations but minimal capability statement integration
- **Poor (2)**: Lists some operations without capability verification
- **Minimal (1)**: No clear FHIR operations specified

**9. Validation Criteria**
- **Perfect (5)**: Specific, measurable validation criteria with clear pass/fail conditions
- **Good (4)**: Good validation criteria with minor gaps in specificity
- **Moderate (3)**: General validation approach but lacks specificity
- **Poor (2)**: Vague validation criteria
- **Minimal (1)**: No clear validation criteria

**10. Markdown Formatting**
- **Perfect (5)**: Flawless markdown with proper headers, formatting, and structure
- **Good (4)**: Good markdown with minor formatting issues
- **Moderate (3)**: Adequate markdown but with notable formatting problems
- **Poor (2)**: Poor markdown formatting affecting readability
- **Minimal (1)**: Little to no proper markdown formatting

**11. Interoperability Focus**
- **Perfect (5)**: Clearly states focus on interoperability conformance and explicitly avoids bug testing
- **Good (4)**: Shows interoperability focus with minor mention of other testing
- **Moderate (3)**: Some interoperability focus but mixed with other concerns
- **Poor (2)**: Limited interoperability focus
- **Minimal (1)**: No clear interoperability focus or focuses on wrong type of testing

**12. Bring Your Own Data Approach**
- **Perfect (5)**: Explicitly describes tests that work with any valid data and avoids specific examples
- **Good (4)**: Generally data-agnostic with minor specific references
- **Moderate (3)**: Some data-agnostic elements but includes specific examples
- **Poor (2)**: Limited data flexibility, relies on specific data
- **Minimal (1)**: Requires specific test data, not data-agnostic

**13. No Fixtures Configuration**
- **Perfect (5)**: Explicitly states no fixtures and demonstrates fixture-free approach
- **Good (4)**: Avoids fixtures but doesn't explicitly mention constraint
- **Moderate (3)**: Mostly avoids fixtures but has some specific configurations
- **Poor (2)**: Includes some fixture-like configurations
- **Minimal (1)**: Includes specific fixture configurations violating instruction

**14. Capability Statement Integration**
- **Perfect (5)**: Extensively references capability statement and validates all operations against it
- **Good (4)**: Good capability statement integration with minor gaps
- **Moderate (3)**: Some capability statement references but limited integration
- **Poor (2)**: Minimal capability statement consideration
- **Minimal (1)**: No meaningful capability statement integration

**15. Appropriate Abstraction Level**
- **Perfect (5)**: Perfect level of detail for interoperability testing - not too specific, not too vague
- **Good (4)**: Generally appropriate level with minor issues
- **Moderate (3)**: Somewhat appropriate but either too detailed or too vague in places
- **Poor (2)**: Poor abstraction level affecting test usability
- **Minimal (1)**: Completely inappropriate abstraction level

### ENHANCED GRADE CALCULATION WITH STRICTER THRESHOLDS

- **A+ (73-75)**: Exceptional performance (97-100%)
- **A (68-72)**: Excellent performance (91-96%)
- **A- (64-67)**: Very good performance (85-90%)
- **B+ (60-63)**: Good performance (80-84%)
- **B (56-59)**: Above average performance (75-79%)
- **B- (52-55)**: Average performance (69-74%)
- **C+ (48-51)**: Below average performance (64-68%)
- **C (44-47)**: Poor performance (59-63%)
- **C- (40-43)**: Very poor performance (53-58%)
- **D (30-39)**: Failing performance (40-52%)
- **F (Below 30)**: Complete failure (Below 40%)

### ENHANCED ANALYSIS REQUIREMENTS

For each category, the evaluator must:
1. **Search for exact compliance** with the specific instruction wording
2. **Look for partial compliance** and note specific deviations
3. **Identify creative interpretations** that may deviate from instructions
4. **Note missing elements** that were explicitly requested
5. **Score harshly** - reserve 5/5 only for truly exceptional adherence
6. **Provide detailed evidence** showing exactly why the score was assigned
7. **Be discriminating** - look for meaningful differences between test plans

### CRITICAL EVALUATION NOTES

- **Default assumption**: Most responses will score 2-4, not 5
- **Perfect scores (5/5)** should be rare and only for exceptional adherence
- **Look for subtle differences** between test plans in the same category
- **Penalize creative interpretation** when strict adherence was requested
- **Reward exact compliance** with specified terminology and structure
- **Be specific about deviations** in evidence quotes

### SPECIFIC INSTRUCTION MAPPING

**1. Requirement ID Section**
- Instruction: "1. Requirement ID"
- Look for: Exact section with requirement identifier

**2. Requirement Analysis Structure**  
- Instruction: "2. Requirement Analysis:"
- Look for: Exact section structure with proper subsections

**3. Testability Classification**
- Instruction: "Testability Assessment: Classify as automatically testable, an attestation, or not testable due to being too vague or covered by the validator"
- Look for: Use of these exact three classification categories

**4. Complexity Assessment**
- Instruction: "Complexity: Simple, Moderate, or Complex"
- Look for: Use of only these three specific complexity levels

**5. Prerequisites Definition**
- Instruction: "Prerequisites: Required system configurations, data, or setup"
- Look for: Clear prerequisites section with appropriate content

**6. Test Implementation Strategy**
- Instruction: "3. Test Implementation Strategy:"
- Look for: Exact section structure as specified

**7. Required Inputs/Outputs**
- Instruction: "Required inputs including required FHIR resources and expected outputs for the test"
- Look for: Clear specification of both inputs AND outputs

**8. Required FHIR Operations**
- Instruction: "Required FHIR Operations: List any specific API calls/operations needed (ensure these are suported in the Capability Statement)"
- Look for: Listed operations AND verification against capability statement

**9. Validation Criteria**
- Instruction: "Validation Criteria: Specific checks to verify conformance"
- Look for: Specific, testable validation criteria

**10. Markdown Formatting**
- Instruction: "Format your response as markdown with clear headers"
- Look for: Proper markdown syntax, headers, formatting

**11. Interoperability Focus**
- Instruction: "focus on testing for interoperability conformance, not testing for bugs or all edge cases"
- Look for: Focus on conformance rather than bug testing

**12. Bring Your Own Data Approach**
- Instruction: "accommodate a 'bring your own data' approach for testing conformance"
- Look for: Generic test descriptions that work with any valid data

**13. No Fixtures Configuration**
- Instruction: "Do not configure fixtures"
- Look for: Absence of specific test data configurations or fixtures

**14. Capability Statement Integration**
- Instruction: "taking into account the relevant Capability Statement information"
- Look for: References to and proper use of the capability statement

**15. Appropriate Abstraction Level**
- Instruction: "Write the test plan at an appropriate level to achieve this"
- Look for: Appropriate level of detail for interoperability testing

## ANALYSIS REQUIREMENTS

For each test plan evaluation:
1. Read the entire test plan thoroughly
2. Search for evidence of each specific instruction
3. Provide exact quotes as evidence (or note "Not found")
4. Score based solely on instruction adherence, not quality
5. Be consistent in scoring across all test plans
6. Focus on what the original prompt specifically requested

Begin your detailed individual analysis of each test plan now.